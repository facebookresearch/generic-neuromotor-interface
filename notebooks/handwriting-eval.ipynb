{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56e257d6-ea5a-4fda-892a-7b4a0fcfb518",
   "metadata": {},
   "source": [
    "# Handwriting decoder evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e59640-652c-46ee-bc15-2bd4abbe9105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from pytorch_lightning import Trainer\n",
    "from hydra import initialize, compose\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from generic_neuromotor_interface.handwriting_utils import CharacterErrorRates\n",
    "\n",
    "TASK_NAME = \"handwriting\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7888a27-a4e0-4d80-9ae5-0bfae2991055",
   "metadata": {},
   "source": [
    "## Establish paths to data and model files\n",
    "\n",
    "Before running this notebook you must make sure to download the data and model checkpoint as follows:\n",
    "```\n",
    "cd ~/generic-neuromotor-interface-data\n",
    "\n",
    "./download_data.sh handriting small_subset <EMG_DATA_DIR>  # or full_data instead of small_subset\n",
    "\n",
    "./download_models.sh handriting <MODELS_DIR>\n",
    "```\n",
    "where `<EMG_DATA_DIR>` and `<MODELS_DIR>` should match the directory specified by the `EMG_DATA_DIR` and `MODELS_DIR` variables defined in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4c1300-9d39-4317-a9cf-4a057d7966b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMG_DATA_DIR = \"~/emg_data/\"  # path to EMG data\n",
    "MODELS_DIR = \"~/emg_models/\"  # path to model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c3bfae-0335-4ef1-b182-e28a0d4cac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.expanduser(EMG_DATA_DIR)):\n",
    "    raise FileNotFoundError(f\"The EMG data path does not exist: {EMG_DATA_DIR}\")\n",
    "\n",
    "if not os.path.exists(os.path.expanduser(MODELS_DIR)):\n",
    "    raise FileNotFoundError(f\"The models path does not exist: {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf3f201-0006-4ec9-a5bf-6a8583a1b450",
   "metadata": {},
   "source": [
    "## Load model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5220619a-3102-49bc-8c8b-e608c8bc9ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load model config\"\"\"\n",
    "\n",
    "config_path = os.path.join(os.path.expanduser(MODELS_DIR), TASK_NAME, \"model_config.yaml\")\n",
    "config = OmegaConf.load(config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40cf1f5-d667-44a4-b7bd-3f0b1af56909",
   "metadata": {},
   "source": [
    "## Load model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19766bb3-a366-4a9b-a118-f9923d343ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load model checkpoint\"\"\"\n",
    "\n",
    "model_ckpt_path = os.path.join(\n",
    "    os.path.expanduser(MODELS_DIR),\n",
    "    TASK_NAME,\n",
    "    \"model_checkpoint.ckpt\"\n",
    ")\n",
    "model = instantiate(config.lightning_module)\n",
    "model = model.load_from_checkpoint(\n",
    "    model_ckpt_path,\n",
    "    map_location=torch.device(\"cpu\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b3b308-0713-419e-9af3-37fa36274909",
   "metadata": {},
   "source": [
    "## Instantiate data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b92765-b05a-49c8-9ac6-ea4a603eaf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Assemble the data module\"\"\"\n",
    "\n",
    "# Update DataModule config with data path\n",
    "config[\"data_module\"][\"data_location\"] = os.path.expanduser(EMG_DATA_DIR)\n",
    "if \"from_csv\" in config[\"data_module\"][\"data_split\"][\"_target_\"]:\n",
    "    config[\"data_module\"][\"data_split\"][\"csv_filename\"] = os.path.join(\n",
    "        os.path.expanduser(EMG_DATA_DIR),\n",
    "        f\"{TASK_NAME}_corpus.csv\"\n",
    "    )\n",
    "\n",
    "datamodule = instantiate(config[\"data_module\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582efd05-02e4-4cdb-9c9d-8ecbd1d1d9f2",
   "metadata": {},
   "source": [
    "## Run inference on one prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef1e0b6-31e5-4106-af02-0e7446f8993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Grab one test prompt\"\"\"\n",
    "\n",
    "test_dataset = datamodule._make_dataset({\"handwriting_user_001_dataset_000\": None}, \"test\")  # from handwriting_mini_split.yaml\n",
    "sample = test_dataset[55]  # an arbitrary prompt from this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a087ba3b-d0b8-433f-9f97-2260d5b201e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run inference\"\"\"\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# unpack sample\n",
    "emg = sample[\"emg\"]\n",
    "labels = sample[\"prompts\"]\n",
    "\n",
    "# compute model outputs\n",
    "with torch.no_grad():\n",
    "    emissions, _slice = model(emg.T.unsqueeze(0))\n",
    "\n",
    "    # compute greedy decode outputs\n",
    "    predictions = model.decoder.decode_batch(\n",
    "        emissions=emissions.movedim(0, 1).numpy(),\n",
    "        emission_lengths=model.network.compute_time_downsampling(\n",
    "            emg_lengths=torch.as_tensor([len(emg)]), slc=_slice\n",
    "        )\n",
    "    )\n",
    "\n",
    "predictions = torch.as_tensor(predictions[0])\n",
    "\n",
    "# convert predictions and labels to characters\n",
    "predictions = model.decoder._charset.labels_to_str(predictions)\n",
    "labels = model.decoder._charset.labels_to_str(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2abd035-9682-4e8f-b209-7178aa7918eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate CER on this prompt\"\"\"\n",
    "\n",
    "metric = CharacterErrorRates()\n",
    "metric.update(\n",
    "    prediction=predictions,\n",
    "    target=labels,\n",
    ")\n",
    "aggregate_metrics = metric.compute()\n",
    "\n",
    "print(\"CER of above prompt decode:\", aggregate_metrics[\"CER\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5059309d-d784-4fc2-bfb3-843a39e67986",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Print predictions and target\"\"\"\n",
    "\n",
    "print(\n",
    "    f\"Prediction: \\t {predictions} \\n\"\n",
    "    f\"Target: \\t {labels}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1673e06-9133-486e-9766-64e55f0be4d3",
   "metadata": {},
   "source": [
    "## Evaluate full test set\n",
    "\n",
    "Note that this requires you to have downloaded the full dataset (`full_data` instead of `small_subset`) when invoking `./download_data.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0d7e8a-2006-46c8-925c-7253007d5060",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(accelerator=\"cpu\")\n",
    "test_results = trainer.test(model=model, datamodule=datamodule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
