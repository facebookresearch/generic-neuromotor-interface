{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56e257d6-ea5a-4fda-892a-7b4a0fcfb518",
   "metadata": {},
   "source": [
    "# Handwriting decoder evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e59640-652c-46ee-bc15-2bd4abbe9105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from pytorch_lightning import Trainer\n",
    "from hydra import initialize, compose\n",
    "from hydra.utils import instantiate\n",
    "import omegaconf\n",
    "\n",
    "from generic_neuromotor_interface.handwriting_utils import CharacterErrorRates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7888a27-a4e0-4d80-9ae5-0bfae2991055",
   "metadata": {},
   "source": [
    "## Establish paths to data and model files\n",
    "\n",
    "Before running this notebook you must make sure to download the data and model checkpoint as follows:\n",
    "```\n",
    "cd ~/generic-neuromotor-interface-data\n",
    "\n",
    "./download_data.sh handriting small_subset <EMG_DATA_DIR>  # or full_data instead of small_subset\n",
    "\n",
    "./download_models.sh handriting <MODELS_DIR>\n",
    "```\n",
    "where `<EMG_DATA_DIR>` and `<MODELS_DIR>` should match the directory specified by the `EMG_DATA_DIR` and `MODELS_DIR` variables defined in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4c1300-9d39-4317-a9cf-4a057d7966b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMG_DATA_DIR = \"~/emg_data/\"  # path to EMG data\n",
    "MODELS_DIR = \"~/emg_models/\"  # path to model files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f685fb01-6d92-449f-9077-efdceaea76e8",
   "metadata": {},
   "source": [
    "## Load model checkpoint and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62520582-959d-4883-9b67-db8df4266bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Retrieve model checkpoint\"\"\"\n",
    "\n",
    "model_ckpt_path = os.path.join(os.path.expanduser(MODELS_DIR), \"handwriting\", \"model_checkpoint.ckpt\")\n",
    "model_ckpt = torch.load(\n",
    "    model_ckpt_path,\n",
    "    map_location=torch.device(\"cpu\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128b6098-7457-4801-957c-2af34091bc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Retrieve the config\"\"\"\n",
    "\n",
    "config_path = os.path.relpath(os.path.join(os.path.expanduser(MODELS_DIR), \"handwriting\"))\n",
    "with initialize(config_path=config_path):\n",
    "    cfg = compose(config_name=\"model_config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b3b308-0713-419e-9af3-37fa36274909",
   "metadata": {},
   "source": [
    "## Instantiate model and data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e724a0-08eb-4607-a9fc-5e272fe75d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load the model\"\"\"\n",
    "\n",
    "# Instantiate model\n",
    "model = instantiate(cfg.lightning_module)\n",
    "\n",
    "# Load the checkpoint state_dict\n",
    "model.load_state_dict(model_ckpt[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b92765-b05a-49c8-9ac6-ea4a603eaf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Assemble the data module\"\"\"\n",
    "\n",
    "# Assemble DataModule config\n",
    "datamodule_cfg = omegaconf.OmegaConf.to_container(cfg.data_module)\n",
    "datamodule_cfg[\"data_location\"] = EMG_DATA_DIR\n",
    "if \"from_csv\" in datamodule_cfg[\"data_split\"][\"_target_\"]:\n",
    "    datamodule_cfg[\"data_split\"][\"csv_filename\"] = os.path.join(EMG_DATA_DIR, \"handwriting_corpus.csv\")\n",
    "\n",
    "# Instantiate DataModule\n",
    "datamodule = instantiate(datamodule_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582efd05-02e4-4cdb-9c9d-8ecbd1d1d9f2",
   "metadata": {},
   "source": [
    "## Run inference on one prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef1e0b6-31e5-4106-af02-0e7446f8993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Grab one test prompt\"\"\"\n",
    "\n",
    "test_dataset = datamodule._make_dataset({\"handwriting_user_002_dataset_000\": None}, \"test\")  # from handwriting_mini_split.yaml\n",
    "sample = test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a087ba3b-d0b8-433f-9f97-2260d5b201e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run inference\"\"\"\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# unpack sample\n",
    "emg = sample[\"emg\"]\n",
    "labels = sample[\"prompts\"]\n",
    "\n",
    "# compute model outputs\n",
    "with torch.no_grad():\n",
    "    emissions, _slice = model(emg.T.unsqueeze(0))\n",
    "\n",
    "    # compute greedy decode outputs\n",
    "    predictions = model.decoder.decode_batch(\n",
    "        emissions=emissions.movedim(0, 1).numpy(),\n",
    "        emission_lengths=model.network.compute_time_downsampling(\n",
    "            emg_lengths=torch.as_tensor([len(emg)]), slc=_slice\n",
    "        )\n",
    "    )\n",
    "\n",
    "predictions = torch.as_tensor(predictions[0])\n",
    "\n",
    "# convert predictions and labels to characters\n",
    "predictions = model.decoder._charset.labels_to_str(predictions)\n",
    "labels = model.decoder._charset.labels_to_str(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2abd035-9682-4e8f-b209-7178aa7918eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate CER on this prompt\"\"\"\n",
    "\n",
    "metric = CharacterErrorRates()\n",
    "metric.update(\n",
    "    prediction=predictions,\n",
    "    target=labels,\n",
    ")\n",
    "aggregate_metrics = metric.compute()\n",
    "\n",
    "print(\"CER of above prompt decode:\", aggregate_metrics[\"CER\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5059309d-d784-4fc2-bfb3-843a39e67986",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Print predictions and target\"\"\"\n",
    "\n",
    "print(\n",
    "    f\"Prediction: \\t {predictions} \\n\"\n",
    "    f\"Target: \\t {labels}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1673e06-9133-486e-9766-64e55f0be4d3",
   "metadata": {},
   "source": [
    "## Evaluate full test set\n",
    "\n",
    "Note that this requires you to have downloaded the full dataset (`full_data` instead of `small_subset`) when invoking `./download_data.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0d7e8a-2006-46c8-925c-7253007d5060",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer()\n",
    "test_results = trainer.test(model=model, datamodule=datamodule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
