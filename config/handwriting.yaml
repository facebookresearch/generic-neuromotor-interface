defaults:
  - _self_
  - data_module: handwriting_data_module
  - data_module/data_split: handwriting_split
  - lightning_module: handwriting_module

# Data settings
data_location: ~/emg_data
download_subset: false

# Dag settings
local: false
name: handwriting-train-dag

# Train settings
seed: 42
train: True # Whether to run training

# TODO: debug this further and clean up?
# XXX: disable gpu-based val/train (hotfix for val hanging issue)
# NOTE: we still have cpu-based val/train in separate component after train
eval: False # Whether to run evaluation on validation and test splits

# Number of nodes to train in DDP (1 node = 1 machine (e.g. G4))
# So 2 nodes ==> 2xG4 ==> 2x8 GPUs ==> 16 GPUs
num_nodes: 1

trainer:
  max_epochs: 200
  strategy: ddp
  accelerator: auto
  gradient_clip_val: 0.1
  gradient_clip_algorithm: 'norm'
  accumulate_grad_batches: 4

# Callback settings
monitor_metric: val/CER
monitor_mode: min

callbacks:
  - _target_: pytorch_lightning.callbacks.LearningRateMonitor
  - _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: ${monitor_metric}
    mode: ${monitor_mode}
    save_last: True
    verbose: True
  - _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: val_loss
    patience: 50

# Customize hydra directory outputs
# see: https://hydra.cc/docs/0.11/configure_hydra/workdir/
hydra:
  run:
    dir: logs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: ${hydra.run.dir}
    subdir: ${hydra.job.num}_${hydra.job.override_dirname}/seed=${seed}
  output_subdir: hydra_configs
  job:
    name: platform-paper-handwriting
    config:
      override_dirname:
        exclude_keys:
          - seed
